{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0b4162"
      },
      "source": [
        "# Task\n",
        "Create a program to read the file \"/content/amenity_analysis_results.csv\", filter out the 1% most expensive houses, one-hot encode the 'Appliance' column and all other string columns, train an XGBoost model to predict the 'Price' column, cross-evaluate the hyperparameters, and then show a list of the highest errors between predicted and actual rent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68e6e2c6"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the specified CSV file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97ee00a"
      },
      "source": [
        "**Reasoning**:\n",
        "Import pandas and load the CSV file into a DataFrame, then display the head and info.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "165c9edc",
        "outputId": "958bb7cb-f682-4ed1-ad60-4e523b7ef31a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "df = pd.read_csv(\"/content/sample_data/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "\n",
        "df.head().to_csv(\"floridaHomeDetailsV3UnEncoded.csv\")\n",
        "\n",
        "# Function to extract zipcode from URL\n",
        "def extract_zipcode_from_url(url):\n",
        "    if isinstance(url, str):\n",
        "        try:\n",
        "            parts = url.split('/homedetails/')[1].split('/')\n",
        "            # Assuming the zipcode is the part before the _zpid\n",
        "            zip_part = parts[-2] if '-' in parts[-2] else parts[-1]\n",
        "            zipcode = zip_part.split('-')[0]\n",
        "            return zipcode\n",
        "        except:\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "# Function to recalculate Average Rental Price\n",
        "def recalculate_average_rent(row):\n",
        "    times_rented = row['Times Rented']\n",
        "    last_rental_price = row['Last Rental Price']\n",
        "    average_time_on_market = row['Average Time on Market'] # Assuming this is the old average\n",
        "\n",
        "    # Check for conditions where calculation is not possible or would result in NaN\n",
        "    if pd.isna(times_rented) or times_rented < 2 or pd.isna(last_rental_price) or pd.isna(average_time_on_market):\n",
        "        return np.nan\n",
        "    else:\n",
        "        # Apply the formula: (Old Average * Times Rented - Last Rental Price) / (Times Rented - 1)\n",
        "        # Need to be careful if Times Rented is 1, as it would result in division by zero.\n",
        "        if times_rented - 1 == 0:\n",
        "            return np.nan # Avoid division by zero\n",
        "        else:\n",
        "            new_average = (average_time_on_market * times_rented - last_rental_price) / (times_rented - 1)\n",
        "            return new_average\n",
        "\n",
        "# Apply the function to fill missing zipcodes\n",
        "df['Zipcode'] = df.apply(\n",
        "    lambda row: extract_zipcode_from_url(row['URL']) if pd.isna(row['Zipcode']) else row['Zipcode'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Apply the function to recalculate Average Time on Market\n",
        "df['Average Time on Market'] = df.apply(recalculate_average_rent, axis=1)\n",
        "\n",
        "# Remove the top 10% of outliers based on 'Most Recent Time on Market'\n",
        "cutoff = df['Price'].quantile(0.95)\n",
        "print(f\"Using a cutoff of {cutoff:.2f} \")\n",
        "\n",
        "# Create a new DataFrame without the top 10% of outliers\n",
        "df = df[df['Price'] <= cutoff]\n",
        "\n",
        "display(df.head())\n",
        "display(df.info())\n",
        "display(df.columns)\n",
        "df['Price'].describe()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/floridaHomeDetailsV3.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-93789715.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/floridaHomeDetailsV3.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"floridaHomeDetailsV3UnEncoded.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/floridaHomeDetailsV3.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5007c97c"
      },
      "source": [
        "# Calculate 'Rent Estimate to actual Price' column\n",
        "df['Rent Estimate to actual Price'] = df['Rent Zestimate'] / df['Price']\n",
        "\n",
        "# Calculate 'price to area average by zipcode' column\n",
        "# First, calculate the average price per zipcode\n",
        "average_price_by_zipcode = df.groupby('Zipcode')['Price'].transform('mean')\n",
        "\n",
        "# Then, calculate the ratio of 'Price' to the average price by zipcode\n",
        "df['price to area average by zipcode'] = df['Price'] / average_price_by_zipcode\n",
        "\n",
        "# Function to extract the numerical month of the most recent 'Listed for rent' or 'Sold' event\n",
        "import json\n",
        "\n",
        "def get_most_recent_month(events_string):\n",
        "    try:\n",
        "        events = json.loads(events_string)\n",
        "        # Filter for 'Listed for rent' or 'Sold' events and sort by date\n",
        "        relevant_events = sorted([e for e in events if e['type'] in ['Listed for rent', 'Sold']],\n",
        "                                 key=lambda x: x['date'], reverse=True)\n",
        "        if relevant_events:\n",
        "            # Get the date of the most recent relevant event\n",
        "            most_recent_date_str = relevant_events[0]['date']\n",
        "            # Extract and return the numerical month\n",
        "            return pd.to_datetime(most_recent_date_str).month\n",
        "    except:\n",
        "        # Return NaN if there's an error parsing or no relevant events\n",
        "        return np.nan\n",
        "    return np.nan\n",
        "\n",
        "# Apply the function to create the 'month listed' column\n",
        "df['month listed'] = df['Events'].apply(get_most_recent_month)\n",
        "\n",
        "display(df[['Rent Estimate to actual Price', 'price to area average by zipcode', 'month listed']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4362de2a"
      },
      "source": [
        "## Filter Data\n",
        "\n",
        "### Subtask:\n",
        "Filter out the 1% most expensive houses based on the 'Price' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "348254a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the 99th percentile of the 'Price' column and filter the DataFrame to exclude houses with prices above this threshold. Display the head and info of the filtered DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca75bbb2"
      },
      "source": [
        "\n",
        "# Function to split and one-hot encode the 'Appliances' column\n",
        "def encode_appliances(df):\n",
        "    # Split the 'Appliances' string into a list of appliances, handling NaN values\n",
        "    appliances_list = df['Appliances'].str.split(', ').apply(lambda x: x if isinstance(x, list) else [])\n",
        "    # Get all unique appliances\n",
        "    all_appliances = sorted(list(set([item for sublist in appliances_list for item in sublist])))\n",
        "    # Create new columns for each appliance and fill with 1 if the appliance is present\n",
        "    for appliance in all_appliances:\n",
        "        df[f'Appliance_{appliance}'] = appliances_list.apply(lambda x: 1 if appliance in x else 0)\n",
        "    return df\n",
        "\n",
        "# Apply one-hot encoding to 'Appliances'\n",
        "df_filtered = encode_appliances(df)\n",
        "\n",
        "# Apply one-hot encoding to other string columns\n",
        "df_filtered = pd.get_dummies(df_filtered, columns=['Cooling', 'Heating', 'Parking', 'Laundry', 'Home Type'])\n",
        "\n",
        "display(df_filtered.head())\n",
        "display(df_filtered.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ab925be"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def fill_nan_with_median(df):\n",
        "    \"\"\"\n",
        "    Fills NaN values in a DataFrame with the median of each column using a manual loop.\n",
        "    If the median cannot be calculated for a column, the NaN values in that column will remain NaN.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for col in df_copy.columns:\n",
        "        try:\n",
        "            # Attempt to calculate the median. Use try-except to catch errors for non-numeric columns\n",
        "            median_val = df_copy[col].median()\n",
        "            if not pd.isna(median_val):\n",
        "                df_copy[col] = df_copy[col].fillna(median_val)\n",
        "        except:\n",
        "            # If median cannot be calculated (e.g., non-numeric column), leave NaN values as they are\n",
        "            pass # Do nothing, leave NaN values\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "# Apply the custom NaN filling function\n",
        "df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "display(df_shuffled.head())\n",
        "display(df_shuffled.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXvcMA_w2wn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_shuffled.head().to_csv(\"floridaHomeDataIncomeEncodedV2.csv\" ,index=False)"
      ],
      "metadata": {
        "id": "ChdaEDpx00TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6ef390"
      },
      "source": [
        "## Train XGBoost Model\n",
        "\n",
        "### Subtask:\n",
        "Train an XGBoost model to predict the 'Price' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9dd0b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate the features (X) and target variable (y), split the data into training and testing sets, and train an XGBoost Regressor model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832c7f2a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "# Exclude non-numeric and irrelevant columns\n",
        "X = df_shuffled.drop(columns=['Price', 'Street Address', 'City', 'State', 'URL', 'Appliances', \"Image URLs\", \"Date Details Fetched\", \"Events\",\"Rent Estimate to actual Price\",\"price to area average by zipcode\"])\n",
        "y = df_shuffled['Price']\n",
        "\n",
        "df_shuffled.to_csv(\"testEncoded.csv\")\n",
        "\n",
        "# Select only numeric columns for training\n",
        "X = X.select_dtypes(include=np.number)\n",
        "\n",
        "# Replace infinite values with NaN and then fill NaN with the median\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "y_test.head().to_csv(\"testOutput.csv\")\n",
        "\n",
        "# Initialize and train the XGBoost Regressor model\n",
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.01, random_state=42)\n",
        "xgbr.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c8003a"
      },
      "source": [
        "## Cross-evaluate Hyperparameters and Evaluate Model\n",
        "\n",
        "### Subtask:\n",
        "Cross-evaluate the hyperparameters of the trained XGBoost model and calculate the R2 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc6bab9"
      },
      "source": [
        "**Reasoning**:\n",
        "Use cross-validation to evaluate the model's performance across different subsets of the data and calculate the R2 score on the test set to assess the model's goodness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca57b379"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Cross-evaluate the model using cross-validation\n",
        "# Using R2 as the scoring metric\n",
        "scores = cross_val_score(xgbr, X_train, y_train, cv=10, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validation R2 scores: {scores}\")\n",
        "print(f\"Mean cross-validation R2 score: {scores.mean()}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgbr.predict(X_test)\n",
        "\n",
        "# Calculate the R2 score on the test set\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"R2 score on the test set: {r2}\")\n",
        "\n",
        "scores = cross_val_score(xgbr, X_train, y_train, cv=10, scoring='neg_mean_absolute_error')\n",
        "\n",
        "print(f\"Cross-validation MAE scores: {scores}\")\n",
        "print(f\"Mean cross-validation MAE score: {scores.mean()}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ca989a"
      },
      "source": [
        "## Analyze Highest Errors\n",
        "\n",
        "### Subtask:\n",
        "Show a list of the highest errors between predicted and actual rent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ptfVy6mMJLhl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYTtyd-ZBj_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cf3e54d"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the absolute errors between the predicted and actual prices, sort the results by error in descending order, and display the instances with the highest errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1691aee6"
      },
      "source": [
        "# Calculate the absolute errors\n",
        "errors = abs(y_test - y_pred)\n",
        "\n",
        "# Create a DataFrame to show actual price, predicted price, and error\n",
        "error_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Error': errors})\n",
        "\n",
        "# Get the original index from the test set\n",
        "error_df = error_df.join(df_shuffled['URL'], how='left')\n",
        "\n",
        "\n",
        "# Sort by error in descending order and display the top errors\n",
        "display(error_df.sort_values(by='Error', ascending=False).head())\n",
        "error_df.sort_values(by='Error', ascending=False).to_csv('error_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fea382b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0262e8e6-5089-48d3-8fe9-3858901d0cc4"
      },
      "source": [
        "%pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "511fae87"
      },
      "source": [
        "## SHAP Analysis\n",
        "\n",
        "### Subtask:\n",
        "Generate a SHAP summary plot to visualize feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e341d41"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the `shap` library to calculate SHAP values for the test set and generate a summary plot to show the impact of each feature on the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bff79cb4"
      },
      "source": [
        "import shap\n",
        "\n",
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(xgbr)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Generate the SHAP summary plot\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "print(df_shuffled['Price'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28e89cc"
      },
      "source": [
        "## Calculate MAE, MSE, and RMSE\n",
        "\n",
        "### Subtask:\n",
        "Calculate and display the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) of the model's predictions in one cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e68119"
      },
      "source": [
        "**Reasoning**:\n",
        "Use `sklearn.metrics.mean_absolute_error`, `sklearn.metrics.mean_squared_error`, and `numpy.sqrt` to calculate MAE, MSE, and RMSE using the actual and predicted values and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "f02b52ca",
        "outputId": "17ebbf87-1548-4dcd-d95d-a3a2b7c3d7da"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-381177542.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Calculate MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean Absolute Error (MAE): {mae}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select only numeric columns from the shuffled DataFrame\n",
        "df_shuffled_numeric = df_shuffled.select_dtypes(include=np.number)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix_shuffled = df_shuffled_numeric.corr()\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(18, 16))\n",
        "\n",
        "# Create the correlation heatmap\n",
        "sns.heatmap(correlation_matrix_shuffled, cmap='coolwarm', annot=False)\n",
        "plt.title('Correlation Heatmap of Numeric Features (Shuffled Data)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "yNmFRiGwfhxC",
        "outputId": "e669ffa8-4e9f-4553-8125-70663d8973db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_shuffled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4232705849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Select only numeric columns from the shuffled DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_shuffled_numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_shuffled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Calculate the correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_shuffled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a scatter plot of actual vs. predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.title('Actual vs. Predicted Price')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "iWfurCaafj13",
        "outputId": "fa00cd13-7b61-4c44-9af1-914dde5d64e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-890455476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a scatter plot of actual vs. predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual vs. Predicted Price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual Price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}