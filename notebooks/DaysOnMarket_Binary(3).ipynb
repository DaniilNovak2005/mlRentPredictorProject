{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Z1y41mzBlo"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn\n",
        "%pip install xgboost\n",
        "%pip install shap\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fb8f702"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "165c9edc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "df = pd.read_csv(\"/csvData/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "\n",
        "# Function to extract zipcode from URL\n",
        "def extract_zipcode_from_url(url):\n",
        "    if isinstance(url, str):\n",
        "        try:\n",
        "            parts = url.split('/homedetails/')[1].split('/')\n",
        "            # Assuming the zipcode is the part before the _zpid\n",
        "            zip_part = parts[-2] if '-' in parts[-2] else parts[-1]\n",
        "            zipcode = zip_part.split('-')[0]\n",
        "            return zipcode\n",
        "        except:\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "# Function to recalculate Average Rental Price\n",
        "def recalculate_average_rent(row):\n",
        "    times_rented = row['Times Rented']\n",
        "    last_rental_price = row['Last Rental Price']\n",
        "    average_time_on_market = row['Average Time on Market'] # Assuming this is the old average\n",
        "\n",
        "    # Check for conditions where calculation is not possible or would result in NaN\n",
        "    if pd.isna(times_rented) or times_rented < 2 or pd.isna(last_rental_price) or pd.isna(average_time_on_market):\n",
        "        return np.nan\n",
        "    else:\n",
        "        # Apply the formula: (Old Average * Times Rented - Last Rental Price) / (Times Rented - 1)\n",
        "        # Need to be careful if Times Rented is 1, as it would result in division by zero.\n",
        "        if times_rented - 1 == 0:\n",
        "            return np.nan # Avoid division by zero\n",
        "        else:\n",
        "            new_average = (average_time_on_market * times_rented - last_rental_price) / (times_rented - 1)\n",
        "            return new_average\n",
        "\n",
        "# Apply the function to fill missing zipcodes\n",
        "df['Zipcode'] = df.apply(\n",
        "    lambda row: extract_zipcode_from_url(row['URL']) if pd.isna(row['Zipcode']) else row['Zipcode'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Apply the function to recalculate Average Time on Market\n",
        "df['Average Time on Market'] = df.apply(recalculate_average_rent, axis=1)\n",
        "\n",
        "# Remove the top 10% of outliers based on 'Most Recent Time on Market'\n",
        "cutoff = df['Most Recent Time on Market'].quantile(0.8)\n",
        "print(f\"Using a cutoff of {cutoff:.2f} days\")\n",
        "\n",
        "# Create a new DataFrame without the top 10% of outliers\n",
        "df = df[df['Most Recent Time on Market'] <= cutoff]\n",
        "\n",
        "display(df.head())\n",
        "display(df.info())\n",
        "df['Most Recent Time on Market'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5007c97c"
      },
      "source": [
        "# Calculate 'Rent Estimate to actual Price' column\n",
        "df['Rent Estimate to actual Price'] = df['Rent Zestimate'] / df['Price']\n",
        "\n",
        "# Calculate 'price to area average by zipcode' column\n",
        "# First, calculate the average price per zipcode\n",
        "average_price_by_zipcode = df.groupby('Zipcode')['Price'].transform('mean')\n",
        "\n",
        "# Then, calculate the ratio of 'Price' to the average price by zipcode\n",
        "df['price to area average by zipcode'] = df['Price'] / average_price_by_zipcode\n",
        "\n",
        "# Function to extract the numerical month of the most recent 'Listed for rent' or 'Sold' event\n",
        "import json\n",
        "\n",
        "def get_most_recent_month(events_string):\n",
        "    try:\n",
        "        events = json.loads(events_string)\n",
        "        # Filter for 'Listed for rent' or 'Sold' events and sort by date\n",
        "        relevant_events = sorted([e for e in events if e['type'] in ['Listed for rent', 'Sold']],\n",
        "                                 key=lambda x: x['date'], reverse=True)\n",
        "        if relevant_events:\n",
        "            # Get the date of the most recent relevant event\n",
        "            most_recent_date_str = relevant_events[0]['date']\n",
        "            # Extract and return the numerical month\n",
        "            return pd.to_datetime(most_recent_date_str).month\n",
        "    except:\n",
        "        # Return NaN if there's an error parsing or no relevant events\n",
        "        return np.nan\n",
        "    return np.nan\n",
        "\n",
        "# Apply the function to create the 'month listed' column\n",
        "df['month listed'] = df['Events'].apply(get_most_recent_month)\n",
        "\n",
        "display(df[['Rent Estimate to actual Price', 'price to area average by zipcode', 'month listed']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4362de2a"
      },
      "source": [
        "## Filter Data\n",
        "\n",
        "### Subtask:\n",
        "Filter out the 1% most expensive houses based on the 'Price' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "348254a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the 99th percentile of the 'Price' column and filter the DataFrame to exclude houses with prices above this threshold. Display the head and info of the filtered DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca75bbb2"
      },
      "source": [
        "\n",
        "# Function to split and one-hot encode the 'Appliances' column\n",
        "def encode_appliances(df):\n",
        "    # Split the 'Appliances' string into a list of appliances, handling NaN values\n",
        "    appliances_list = df['Appliances'].str.split(', ').apply(lambda x: x if isinstance(x, list) else [])\n",
        "    # Get all unique appliances\n",
        "    all_appliances = sorted(list(set([item for sublist in appliances_list for item in sublist])))\n",
        "    # Create new columns for each appliance and fill with 1 if the appliance is present\n",
        "    for appliance in all_appliances:\n",
        "        df[f'Appliance_{appliance}'] = appliances_list.apply(lambda x: 1 if appliance in x else 0)\n",
        "    return df\n",
        "\n",
        "# Apply one-hot encoding to 'Appliances'\n",
        "df_filtered = encode_appliances(df)\n",
        "\n",
        "# Apply one-hot encoding to other string columns\n",
        "df_filtered = pd.get_dummies(df_filtered, columns=['Cooling', 'Heating', 'Parking', 'Laundry', 'Home Type'])\n",
        "\n",
        "display(df_filtered.head())\n",
        "display(df_filtered.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ab925be"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def fill_nan_with_median(df):\n",
        "    \"\"\"\n",
        "    Fills NaN values in a DataFrame with the median of each column using a manual loop.\n",
        "    If the median cannot be calculated for a column, the NaN values in that column will remain NaN.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for col in df_copy.columns:\n",
        "        try:\n",
        "            # Attempt to calculate the median. Use try-except to catch errors for non-numeric columns\n",
        "            median_val = df_copy[col].median()\n",
        "            if not pd.isna(median_val):\n",
        "                df_copy[col] = df_copy[col].fillna(median_val)\n",
        "        except:\n",
        "            # If median cannot be calculated (e.g., non-numeric column), leave NaN values as they are\n",
        "            pass # Do nothing, leave NaN values\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "# Apply the custom NaN filling function\n",
        "df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "display(df_shuffled.head())\n",
        "display(df_shuffled.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6ef390"
      },
      "source": [
        "## Train XGBoost Model\n",
        "\n",
        "### Subtask:\n",
        "Train an XGBoost model to predict the 'Price' column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9dd0b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate the features (X) and target variable (y), split the data into training and testing sets, and train an XGBoost Regressor model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832c7f2a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "# Exclude non-numeric and irrelevant columns\n",
        "X = df_shuffled.drop(columns=['Most Recent Time on Market', 'Street Address', 'City', 'State', 'URL', 'Appliances', \"Image URLs\", \"Date Details Fetched\", \"Events\", \"Price\"])\n",
        "y = df_shuffled['Most Recent Time on Market']\n",
        "\n",
        "# Select only numeric columns for training\n",
        "X = X.select_dtypes(include=np.number)\n",
        "\n",
        "# Replace infinite values with NaN and then fill NaN with the median\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "y_test.head().to_csv(\"testOutput.csv\")\n",
        "\n",
        "# Initialize and train the XGBoost Regressor model\n",
        "xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=400, learning_rate=0.01, random_state=42)\n",
        "xgbr.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c8003a"
      },
      "source": [
        "## Cross-evaluate Hyperparameters and Evaluate Model\n",
        "\n",
        "### Subtask:\n",
        "Cross-evaluate the hyperparameters of the trained XGBoost model and calculate the R2 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc6bab9"
      },
      "source": [
        "**Reasoning**:\n",
        "Use cross-validation to evaluate the model's performance across different subsets of the data and calculate the R2 score on the test set to assess the model's goodness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca57b379"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Cross-evaluate the model using cross-validation\n",
        "# Using R2 as the scoring metric\n",
        "scores = cross_val_score(xgbr, X_train, y_train, cv=10, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validation R2 scores: {scores}\")\n",
        "print(f\"Mean cross-validation R2 score: {scores.mean()}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgbr.predict(X_test)\n",
        "\n",
        "# Calculate the R2 score on the test set\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"R2 score on the test set: {r2}\")\n",
        "\n",
        "scores = cross_val_score(xgbr, X_train, y_train, cv=10, scoring='neg_mean_absolute_error')\n",
        "\n",
        "print(f\"Cross-validation MAE scores: {scores}\")\n",
        "print(f\"Mean cross-validation MAE score: {scores.mean()}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ca989a"
      },
      "source": [
        "## Analyze Highest Errors\n",
        "\n",
        "### Subtask:\n",
        "Show a list of the highest errors between predicted and actual rent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ptfVy6mMJLhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgbr.save_model(\"flDaysModel.json\")"
      ],
      "metadata": {
        "id": "pYTtyd-ZBj_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cf3e54d"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the absolute errors between the predicted and actual prices, sort the results by error in descending order, and display the instances with the highest errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1691aee6"
      },
      "source": [
        "# Calculate the absolute errors\n",
        "errors = abs(y_test - y_pred)\n",
        "\n",
        "# Create a DataFrame to show actual price, predicted price, and error\n",
        "error_df = pd.DataFrame({'Actual Days on Zillow': y_test, 'Predicted': y_pred, 'Error': errors})\n",
        "\n",
        "# Get the original index from the test set\n",
        "error_df = error_df.join(df_shuffled['URL'], how='left')\n",
        "\n",
        "\n",
        "# Sort by error in descending order and display the top errors\n",
        "display(error_df.sort_values(by='Error', ascending=False).head())\n",
        "error_df.sort_values(by='Error', ascending=False).to_csv('error_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fea382b"
      },
      "source": [
        "%pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "511fae87"
      },
      "source": [
        "## SHAP Analysis\n",
        "\n",
        "### Subtask:\n",
        "Generate a SHAP summary plot to visualize feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e341d41"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the `shap` library to calculate SHAP values for the test set and generate a summary plot to show the impact of each feature on the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bff79cb4"
      },
      "source": [
        "import shap\n",
        "\n",
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(xgbr)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Generate the SHAP summary plot\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "print(df_shuffled['Most Recent Time on Market'].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28e89cc"
      },
      "source": [
        "## Calculate MAE, MSE, and RMSE\n",
        "\n",
        "### Subtask:\n",
        "Calculate and display the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) of the model's predictions in one cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e68119"
      },
      "source": [
        "**Reasoning**:\n",
        "Use `sklearn.metrics.mean_absolute_error`, `sklearn.metrics.mean_squared_error`, and `numpy.sqrt` to calculate MAE, MSE, and RMSE using the actual and predicted values and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f02b52ca"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcdfd68"
      },
      "source": [
        "# Task\n",
        "Read \"floridaHomeDetailsIncome.csv\", sort it by Latitude then Longitude, calculate MAE and RMSE for the XGBoost model, generate a Shap summary plot, and create a function that takes the dataframe, the trained XGBoost model, and a dictionary representing a new house, predicts the price of the new house by filling missing values from the closest house in the dataframe and one-hot encoding the features, and returns the predicted price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff6368de"
      },
      "source": [
        "## Define the new house dictionary\n",
        "\n",
        "### Subtask:\n",
        "Define the new house dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c759683"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a dictionary for the new house with relevant features for prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bee0fcf"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine all the necessary steps for predicting the price of a new house into one code cell for clarity and to ensure consistent data handling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgbr.save_model(\"xgbModelFloridaV2.json\")"
      ],
      "metadata": {
        "id": "AGMvIdGM3sdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a9f473"
      },
      "source": [
        "# Task\n",
        "Edit the copy to perform binary classification to determine whether a house will sell fast (< 30 days) or not, evaluate the model's performance using appropriate metrics, and analyze feature importance using SHAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ae416c"
      },
      "source": [
        "## Modify data preparation\n",
        "\n",
        "### Subtask:\n",
        "Create a new target variable for binary classification based on 'Most Recent Time on Market' (e.g., 1 for < 30 days, 0 otherwise).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54f1916"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the binary target variable 'is_fast_sale' based on the 'Most Recent Time on Market' column, display the head and info of the updated DataFrame, and show the value counts of the new target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aad926"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Try loading from a common Google Drive path\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/csvData/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    # If that fails, try the original path (in case the environment is different)\n",
        "    try:\n",
        "        df = pd.read_csv(\"csvData/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "    except FileNotFoundError:\n",
        "        # If that fails, try the path specified in the notebook description\n",
        "        try:\n",
        "            df = pd.read_csv(\"/content/amenity_analysis_results.csv\", on_bad_lines='skip')\n",
        "        except FileNotFoundError:\n",
        "            print(\"Error: Data file not found in common paths.\")\n",
        "            # Set df to None or an empty DataFrame to prevent further errors\n",
        "            df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Function to extract zipcode from URL\n",
        "    def extract_zipcode_from_url(url):\n",
        "        if isinstance(url, str):\n",
        "            try:\n",
        "                parts = url.split('/homedetails/')[1].split('/')\n",
        "                # Assuming the zipcode is the part before the _zpid\n",
        "                zip_part = parts[-2] if '-' in parts[-2] else parts[-1]\n",
        "                zipcode = zip_part.split('-')[0]\n",
        "                return zipcode\n",
        "            except:\n",
        "                return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    # Function to recalculate Average Rental Price\n",
        "    def recalculate_average_rent(row):\n",
        "        times_rented = row['Times Rented']\n",
        "        last_rental_price = row['Last Rental Price']\n",
        "        average_time_on_market = row['Average Time on Market'] # Assuming this is the old average\n",
        "\n",
        "        # Check for conditions where calculation is not possible or would result in NaN\n",
        "        if pd.isna(times_rented) or times_rented < 2 or pd.isna(last_rental_price) or pd.isna(average_time_on_market):\n",
        "            return np.nan\n",
        "        else:\n",
        "            # Apply the formula: (Old Average * Times Rented - Last Rental Price) / (Times Rented - 1)\n",
        "            # Need to be careful if Times Rented is 1, as it would result in division by zero.\n",
        "            if times_rented - 1 == 0:\n",
        "                return np.nan # Avoid division by zero\n",
        "            else:\n",
        "                new_average = (average_time_on_market * times_rented - last_rental_price) / (times_rented - 1)\n",
        "                return new_average\n",
        "\n",
        "    # Apply the function to fill missing zipcodes\n",
        "    df['Zipcode'] = df.apply(\n",
        "        lambda row: extract_zipcode_from_url(row['URL']) if pd.isna(row['Zipcode']) else row['Zipcode'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Apply the function to recalculate Average Time on Market\n",
        "    df['Average Time on Market'] = df.apply(recalculate_average_rent, axis=1)\n",
        "\n",
        "    # Remove the top 10% of outliers based on 'Most Recent Time on Market'\n",
        "    cutoff = df['Most Recent Time on Market'].quantile(0.8)\n",
        "    df = df[df['Most Recent Time on Market'] <= cutoff].copy() # Added .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Calculate 'Rent Estimate to actual Price' column\n",
        "    df['Rent Estimate to actual Price'] = df['Rent Zestimate'] / df['Price']\n",
        "\n",
        "    # Calculate 'price to area average by zipcode' column\n",
        "    # First, calculate the average price per zipcode\n",
        "    average_price_by_zipcode = df.groupby('Zipcode')['Price'].transform('mean')\n",
        "\n",
        "    # Then, calculate the ratio of 'Price' to the average price by zipcode\n",
        "    df['price to area average by zipcode'] = df['Price'] / average_price_by_zipcode\n",
        "\n",
        "    # Function to extract the numerical month of the most recent 'Listed for rent' or 'Sold' event\n",
        "    def get_most_recent_month(events_string):\n",
        "        try:\n",
        "            events = json.loads(events_string)\n",
        "            # Filter for 'Listed for rent' or 'Sold' events and sort by date\n",
        "            relevant_events = sorted([e for e in events if e['type'] in ['Listed for rent', 'Sold']],\n",
        "                                     key=lambda x: x['date'], reverse=True)\n",
        "            if relevant_events:\n",
        "                # Get the date of the most recent relevant event\n",
        "                most_recent_date_str = relevant_events[0]['date']\n",
        "                # Extract and return the numerical month\n",
        "                return pd.to_datetime(most_recent_date_str).month\n",
        "        except:\n",
        "            # Return NaN if there's an error parsing or no relevant events\n",
        "            return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    # Apply the function to create the 'month listed' column\n",
        "    df['month listed'] = df['Events'].apply(get_most_recent_month)\n",
        "\n",
        "\n",
        "    # Filter out the 1% most expensive houses\n",
        "    price_99th_percentile = df['Price'].quantile(0.99)\n",
        "    df_filtered = df[df['Price'] <= price_99th_percentile].copy() # Added .copy()\n",
        "\n",
        "    # Function to split and one-hot encode the 'Appliances' column\n",
        "    def encode_appliances(df):\n",
        "        # Split the 'Appliances' string into a list of appliances, handling NaN values\n",
        "        appliances_list = df['Appliances'].str.split(', ').apply(lambda x: x if isinstance(x, list) else [])\n",
        "        # Get all unique appliances\n",
        "        all_appliances = sorted(list(set([item for sublist in appliances_list for item in sublist])))\n",
        "        # Create new columns for each appliance and fill with 1 if the appliance is present\n",
        "        for appliance in all_appliances:\n",
        "            df[f'Appliance_{appliance}'] = appliances_list.apply(lambda x: 1 if appliance in x else 0)\n",
        "        return df\n",
        "\n",
        "    # Apply one-hot encoding to 'Appliances'\n",
        "    df_filtered = encode_appliances(df_filtered)\n",
        "\n",
        "    # Apply one-hot encoding to other string columns\n",
        "    df_filtered = pd.get_dummies(df_filtered, columns=['Cooling', 'Heating', 'Parking', 'Laundry', 'Home Type'])\n",
        "\n",
        "    # Function to fill NaN values with the median\n",
        "    def fill_nan_with_median(df):\n",
        "        df_copy = df.copy()\n",
        "        for col in df_copy.columns:\n",
        "            try:\n",
        "                median_val = df_copy[col].median()\n",
        "                if not pd.isna(median_val):\n",
        "                    df_copy[col] = df_copy[col].fillna(median_val)\n",
        "            except:\n",
        "                pass\n",
        "        return df_copy\n",
        "\n",
        "    # Apply the custom NaN filling function\n",
        "    df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "    # Shuffle the DataFrame\n",
        "    df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "    # Create the binary target variable\n",
        "    df_shuffled['is_fast_sale'] = (df_shuffled['Most Recent Time on Market'] < 30).astype(int)\n",
        "\n",
        "    # Display the head of the DataFrame with the new column\n",
        "    display(df_shuffled.head())\n",
        "\n",
        "    # Display the data types\n",
        "    display(df_shuffled.info())\n",
        "\n",
        "    # Display the count of each unique value in 'is_fast_sale'\n",
        "    display(df_shuffled['is_fast_sale'].value_counts())\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Skipping subsequent steps.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b83afaa"
      },
      "source": [
        "**Reasoning**:\n",
        "The variable `df_shuffled` was not defined because the previous code cell that created and shuffled the dataframe was not executed in the current runtime. To fix this, I will re-execute the code cell that defines and shuffles the dataframe before creating the new target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEoVdm7dewew"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def fill_nan_with_median(df):\n",
        "    \"\"\"\n",
        "    Fills NaN values in a DataFrame with the median of each column using a manual loop.\n",
        "    If the median cannot be calculated for a column, the NaN values in that column will remain NaN.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for col in df_copy.columns:\n",
        "        try:\n",
        "            # Attempt to calculate the median. Use try-except to catch errors for non-numeric columns\n",
        "            median_val = df_copy[col].median()\n",
        "            if not pd.isna(median_val):\n",
        "                df_copy[col] = df_copy[col].fillna(median_val)\n",
        "        except:\n",
        "            # If median cannot be calculated (e.g., non-numeric column), leave NaN values as they are\n",
        "            pass # Do nothing, leave NaN values\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "# Apply the custom NaN filling function\n",
        "df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "# Create the binary target variable\n",
        "df_shuffled['is_fast_sale'] = (df_shuffled['Most Recent Time on Market'] < 30).astype(int)\n",
        "\n",
        "# Display the head of the DataFrame with the new column\n",
        "display(df_shuffled.head())\n",
        "\n",
        "# Display the data types\n",
        "display(df_shuffled.info())\n",
        "\n",
        "# Display the count of each unique value in 'is_fast_sale'\n",
        "display(df_shuffled['is_fast_sale'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13c0b63"
      },
      "source": [
        "**Reasoning**:\n",
        "The variable `df_filtered` was not defined because the previous code cell that created and filtered the dataframe was not executed in the current runtime. To fix this, I will re-execute the code cell that filters and performs one-hot encoding on the dataframe before creating the new target variable. I will also include the code from the subsequent cells that create `df_shuffled` and the `is_fast_sale` column to complete the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqLcfNnXeziw"
      },
      "source": [
        "# Function to split and one-hot encode the 'Appliances' column\n",
        "def encode_appliances(df):\n",
        "    # Split the 'Appliances' string into a list of appliances, handling NaN values\n",
        "    appliances_list = df['Appliances'].str.split(', ').apply(lambda x: x if isinstance(x, list) else [])\n",
        "    # Get all unique appliances\n",
        "    all_appliances = sorted(list(set([item for sublist in appliances_list for item in sublist])))\n",
        "    # Create new columns for each appliance and fill with 1 if the appliance is present\n",
        "    for appliance in all_appliances:\n",
        "        df[f'Appliance_{appliance}'] = appliances_list.apply(lambda x: 1 if appliance in x else 0)\n",
        "    return df\n",
        "\n",
        "# Apply one-hot encoding to 'Appliances'\n",
        "df_filtered = encode_appliances(df)\n",
        "\n",
        "# Apply one-hot encoding to other string columns\n",
        "df_filtered = pd.get_dummies(df_filtered, columns=['Cooling', 'Heating', 'Parking', 'Laundry', 'Home Type'])\n",
        "\n",
        "# Function to fill NaN values with the median\n",
        "def fill_nan_with_median(df):\n",
        "    df_copy = df.copy()\n",
        "    for col in df_copy.columns:\n",
        "        try:\n",
        "            median_val = df_copy[col].median()\n",
        "            if not pd.isna(median_val):\n",
        "                df_copy[col] = df_copy[col].fillna(median_val)\n",
        "        except:\n",
        "            pass\n",
        "    return df_copy\n",
        "\n",
        "# Apply the custom NaN filling function\n",
        "df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "# Create the binary target variable\n",
        "df_shuffled['is_fast_sale'] = (df_shuffled['Most Recent Time on Market'] < 30).astype(int)\n",
        "\n",
        "# Display the head of the DataFrame with the new column\n",
        "display(df_shuffled.head())\n",
        "\n",
        "# Display the data types\n",
        "display(df_shuffled.info())\n",
        "\n",
        "# Display the count of each unique value in 'is_fast_sale'\n",
        "display(df_shuffled['is_fast_sale'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0490b6c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The variable `df` was not defined because the previous code cell that loaded the data was not executed in the current runtime. To fix this, I will re-execute the code cell that loads the data and performs initial processing before filtering, one-hot encoding, filling NaNs, shuffling, and creating the new target variable. This will ensure that all necessary data and variables are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338b7964"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the data failed because the file path was incorrect. Based on the file listing, the data file is not directly in '/content/' or '/content/sample_data/'. Since the original notebook attempted to mount Google Drive, it's likely the data is located there. I will try loading the data from a common Google Drive path. I will include the rest of the code for the subtask as well, assuming the data load is successful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94f1b184"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Try loading from a common Google Drive path\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/csvData/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "except FileNotFoundError:\n",
        "    # If that fails, try the original path (in case the environment is different)\n",
        "    try:\n",
        "        df = pd.read_csv(\"csvData/floridaHomeDetailsV3.csv\",on_bad_lines='skip')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Data file not found in common paths.\")\n",
        "        # Set df to None or an empty DataFrame to prevent further errors\n",
        "        df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Function to extract zipcode from URL\n",
        "    def extract_zipcode_from_url(url):\n",
        "        if isinstance(url, str):\n",
        "            try:\n",
        "                parts = url.split('/homedetails/')[1].split('/')\n",
        "                # Assuming the zipcode is the part before the _zpid\n",
        "                zip_part = parts[-2] if '-' in parts[-2] else parts[-1]\n",
        "                zipcode = zip_part.split('-')[0]\n",
        "                return zipcode\n",
        "            except:\n",
        "                return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    # Function to recalculate Average Rental Price\n",
        "    def recalculate_average_rent(row):\n",
        "        times_rented = row['Times Rented']\n",
        "        last_rental_price = row['Last Rental Price']\n",
        "        average_time_on_market = row['Average Time on Market'] # Assuming this is the old average\n",
        "\n",
        "        # Check for conditions where calculation is not possible or would result in NaN\n",
        "        if pd.isna(times_rented) or times_rented < 2 or pd.isna(last_rental_price) or pd.isna(average_time_on_market):\n",
        "            return np.nan\n",
        "        else:\n",
        "            # Apply the formula: (Old Average * Times Rented - Last Rental Price) / (Times Rented - 1)\n",
        "            # Need to be careful if Times Rented is 1, as it would result in division by zero.\n",
        "            if times_rented - 1 == 0:\n",
        "                return np.nan # Avoid division by zero\n",
        "            else:\n",
        "                new_average = (average_time_on_market * times_rented - last_rental_price) / (times_rented - 1)\n",
        "                return new_average\n",
        "\n",
        "    # Apply the function to fill missing zipcodes\n",
        "    df['Zipcode'] = df.apply(\n",
        "        lambda row: extract_zipcode_from_url(row['URL']) if pd.isna(row['Zipcode']) else row['Zipcode'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Apply the function to recalculate Average Time on Market\n",
        "    df['Average Time on Market'] = df.apply(recalculate_average_rent, axis=1)\n",
        "\n",
        "    # Remove the top 10% of outliers based on 'Most Recent Time on Market'\n",
        "    cutoff = df['Most Recent Time on Market'].quantile(0.8)\n",
        "    #df = df[df['Most Recent Time on Market'] <= cutoff].copy() # Added .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Calculate 'Rent Estimate to actual Price' column\n",
        "    df['Rent Estimate to actual Price'] = df['Rent Zestimate'] / df['Price']\n",
        "\n",
        "    # Calculate 'price to area average by zipcode' column\n",
        "    # First, calculate the average price per zipcode\n",
        "    average_price_by_zipcode = df.groupby('Zipcode')['Price'].transform('mean')\n",
        "\n",
        "    # Then, calculate the ratio of 'Price' to the average price by zipcode\n",
        "    df['price to area average by zipcode'] = df['Price'] / average_price_by_zipcode\n",
        "\n",
        "    # Function to extract the numerical month of the most recent 'Listed for rent' or 'Sold' event\n",
        "    def get_most_recent_month(events_string):\n",
        "        try:\n",
        "            events = json.loads(events_string)\n",
        "            # Filter for 'Listed for rent' or 'Sold' events and sort by date\n",
        "            relevant_events = sorted([e for e in events if e['type'] in ['Listed for rent', 'Sold']],\n",
        "                                     key=lambda x: x['date'], reverse=True)\n",
        "            if relevant_events:\n",
        "                # Get the date of the most recent relevant event\n",
        "                most_recent_date_str = relevant_events[0]['date']\n",
        "                # Extract and return the numerical month\n",
        "                return pd.to_datetime(most_recent_date_str).month\n",
        "        except:\n",
        "            # Return NaN if there's an error parsing or no relevant events\n",
        "            return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    # Apply the function to create the 'month listed' column\n",
        "    df['month listed'] = df['Events'].apply(get_most_recent_month)\n",
        "\n",
        "\n",
        "    # Filter out the 1% most expensive houses\n",
        "    price_99th_percentile = df['Most Recent Time on Market'].quantile(0.8)\n",
        "    df_filtered = df[df['Most Recent Time on Market'] <= price_99th_percentile].copy() # Added .copy()\n",
        "\n",
        "    # Function to split and one-hot encode the 'Appliances' column\n",
        "    def encode_appliances(df):\n",
        "        # Split the 'Appliances' string into a list of appliances, handling NaN values\n",
        "        appliances_list = df['Appliances'].str.split(', ').apply(lambda x: x if isinstance(x, list) else [])\n",
        "        # Get all unique appliances\n",
        "        all_appliances = sorted(list(set([item for sublist in appliances_list for item in sublist])))\n",
        "        # Create new columns for each appliance and fill with 1 if the appliance is present\n",
        "        for appliance in all_appliances:\n",
        "            df[f'Appliance_{appliance}'] = appliances_list.apply(lambda x: 1 if appliance in x else 0)\n",
        "        return df\n",
        "\n",
        "    # Apply one-hot encoding to 'Appliances'\n",
        "    df_filtered = encode_appliances(df_filtered)\n",
        "\n",
        "    # Apply one-hot encoding to other string columns\n",
        "    df_filtered = pd.get_dummies(df_filtered, columns=['Cooling', 'Heating', 'Parking', 'Laundry', 'Home Type'])\n",
        "\n",
        "    # Function to fill NaN values with the median\n",
        "    def fill_nan_with_median(df):\n",
        "        df_copy = df.copy()\n",
        "        for col in df_copy.columns:\n",
        "            try:\n",
        "                median_val = df_copy[col].median()\n",
        "                if not pd.isna(median_val):\n",
        "                    df_copy[col] = df_copy[col].fillna(median_val)\n",
        "            except:\n",
        "                pass\n",
        "        return df_copy\n",
        "\n",
        "    # Apply the custom NaN filling function\n",
        "    df_filled = fill_nan_with_median(df_filtered.copy())\n",
        "\n",
        "    # Shuffle the DataFrame\n",
        "    df_shuffled = df_filled.sample(frac=1, random_state=41).reset_index(drop=True)\n",
        "\n",
        "    # Create the binary target variable\n",
        "    df_shuffled['is_fast_sale'] = (df_shuffled['Most Recent Time on Market'] < 30).astype(int)\n",
        "\n",
        "    # Display the head of the DataFrame with the new column\n",
        "    display(df_shuffled.head())\n",
        "\n",
        "    # Display the data types\n",
        "    display(df_shuffled.info())\n",
        "\n",
        "    # Display the count of each unique value in 'is_fast_sale'\n",
        "    display(df_shuffled['is_fast_sale'].value_counts())\n",
        "else:\n",
        "    print(\"DataFrame not loaded. Skipping subsequent steps.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bf29e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The data file was not found in the previously attempted paths. Since the original notebook attempted to mount Google Drive, I will assume the file is accessible through that method and attempt to load it from the path specified in the notebook description: \"/content/amenity_analysis_results.csv\". I will then proceed with the steps for the current subtask, which is to create the binary target variable 'is_fast_sale'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ef4bc3"
      },
      "source": [
        "**Reasoning**:\n",
        "The data file was not found at the specified path. I need to check the available files again to determine the correct file path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7a086aa"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content/'))\n",
        "print(os.listdir('/content/sample_data/'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b156cf5f"
      },
      "source": [
        "## Modify model training\n",
        "\n",
        "### Subtask:\n",
        "Change the XGBoost model to a classifier (`xgb.XGBClassifier`) and train it using the new binary target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1138ff7"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate features and the new binary target, select numeric columns, handle missing values, split the data, initialize and train the XGBoost classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ee8c5d4"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Separate features (X) and the new binary target variable (y)\n",
        "# Exclude non-numeric, irrelevant columns, and the original target 'Price'\n",
        "X = df_shuffled.drop(columns=['Most Recent Time on Market', 'Price', 'Street Address', 'City', 'State', 'URL', 'Appliances', \"Image URLs\", \"Date Details Fetched\", \"Events\", 'is_fast_sale'])\n",
        "y = df_shuffled['is_fast_sale']\n",
        "\n",
        "# Select only numeric columns for training\n",
        "X = X.select_dtypes(include=np.number)\n",
        "\n",
        "# Replace infinite values with NaN and then fill NaN with the median\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify for binary classification\n",
        "\n",
        "# Initialize and train the XGBoost Classifier model\n",
        "# Using objective='binary:logistic' for binary classification\n",
        "# use_label_encoder=False is used to suppress a deprecation warning\n",
        "# eval_metric='logloss' is a common metric for binary classification\n",
        "xgbc = xgb.XGBClassifier(objective='binary:logistic', n_estimators=400, learning_rate=0.01, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"XGBoost classification model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6d7423"
      },
      "source": [
        "## Evaluate Classification Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the model's performance using appropriate metrics for binary classification (e.g., accuracy, precision, recall, F1-score, ROC AUC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5df821b"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and display common classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC to assess the performance of the trained XGBoost classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6855b73b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgbc.predict(X_test)\n",
        "y_pred_proba = xgbc.predict_proba(X_test)[:, 1] # Get the probabilities for the positive class\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "display(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afad8f4"
      },
      "source": [
        "## SHAP Analysis for Classification\n",
        "\n",
        "### Subtask:\n",
        "Generate a SHAP summary plot to visualize feature importance for the classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065dd0b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the `shap` library to calculate SHAP values for the test set and generate a summary plot to show the impact of each feature on the classification model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d750d5c1"
      },
      "source": [
        "import shap\n",
        "\n",
        "# Create a SHAP explainer object for the classification model\n",
        "explainer = shap.TreeExplainer(xgbc)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "# For classification, shap_values will be a list of arrays, one for each class\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Generate the SHAP summary plot for the positive class (class 1: fast sale)\n",
        "# We use shap_values[1] to get the SHAP values for the positive class\n",
        "shap.summary_plot(shap_values[1], X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d6d940d"
      },
      "source": [
        "# Task\n",
        "Generate a scatter plot of predicted vs actual values, fix the `ValueError: could not convert string to float: '[3.8168877E-1]'` in the SHAP graph, and create a correlation heatmap using the data in \"/content/floridaHomeDetailsV3.csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b681b76a"
      },
      "source": [
        "## Fix shap error\n",
        "\n",
        "### Subtask:\n",
        "Identify and convert any remaining non-numeric data in the feature set `X_test` to a numeric type to resolve the `ValueError` in the SHAP analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2fad2c9"
      },
      "source": [
        "**Reasoning**:\n",
        "The SHAP analysis failed because of a `ValueError` indicating a string could not be converted to a float. This suggests there are non-numeric values remaining in the `X_test` DataFrame that the SHAP explainer cannot handle. I will iterate through the columns of `X_test`, attempt to convert them to numeric, and then inspect any columns that still have non-numeric data to identify the cause of the error and apply appropriate cleaning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32b7d536"
      },
      "source": [
        "**Reasoning**:\n",
        "The scatter plot has been generated. The next subtask is to fix the ValueError in the SHAP graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ae231aa"
      },
      "source": [
        "## Generate Scatter Plot of Predicted vs. Actual\n",
        "\n",
        "**Reasoning**:\n",
        "Create a scatter plot to visualize the relationship between the actual and predicted 'is_fast_sale' values from the classification model. This will help assess the model's performance visually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d68fdd8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a scatter plot of actual vs. predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.title('Actual vs. Predicted Days on Market')\n",
        "plt.xlabel('Actual Days on Market')\n",
        "plt.ylabel('Predicted Days on Market')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15dd240c"
      },
      "source": [
        "## Generate Correlation Heatmap\n",
        "\n",
        "**Reasoning**:\n",
        "Calculate the correlation matrix for the relevant numeric columns in the DataFrame `df_shuffled` and generate a heatmap to visualize the correlations. This will help understand the relationships between the features used in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dadc2f36"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select only numeric columns from the shuffled DataFrame\n",
        "df_shuffled_numeric = df_shuffled.select_dtypes(include=np.number)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix_shuffled = df_shuffled_numeric.corr()\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(18, 16))\n",
        "\n",
        "# Create the correlation heatmap\n",
        "sns.heatmap(correlation_matrix_shuffled, cmap='coolwarm', annot=False)\n",
        "plt.title('Correlation Heatmap of Numeric Features (Shuffled Data)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}